{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to Deep Learning Assignment questions**"
      ],
      "metadata": {
        "id": "wMOK25RpZfOQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyQDBrWQZVaA",
        "outputId": "056a43f8-4076-4014-8117-e4d95683f158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8644 - loss: 0.4881 - val_accuracy: 0.9591 - val_loss: 0.1320\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9638 - loss: 0.1230 - val_accuracy: 0.9673 - val_loss: 0.1025\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9747 - loss: 0.0817 - val_accuracy: 0.9730 - val_loss: 0.0869\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9830 - loss: 0.0555 - val_accuracy: 0.9739 - val_loss: 0.0812\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.9862 - loss: 0.0447 - val_accuracy: 0.9744 - val_loss: 0.0835\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9710 - loss: 0.0971\n",
            "Test accuracy: 0.974399983882904\n"
          ]
        }
      ],
      "source": [
        "# 1. Explain what deep learning is and discuss its significance in the broader field of artificial intelligence.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the deep learning model\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),  # Flatten the 28x28 images into 1D\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')  # 10 classes for MNIST digits (0-9)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning is a subfield of machine learning in which artificial neural networks (ANNs) are used to model and solve complex problems by automatically learning patterns from large amounts of data. Unlike traditional machine learning techniques that require manual feature engineering, deep learning allows models to automatically discover features through multiple layers of abstraction. This process is inspired by the human brain's structure, where neurons are interconnected and process information in layers.\n",
        "\n",
        "Deep learning has revolutionized many AI applications due to its ability to handle unstructured data, such as images, audio, and text, and perform tasks with impressive accuracy. It's widely used in areas such as computer vision, natural language processing, autonomous vehicles, healthcare, and robotics.\n",
        "\n",
        "**Significance in Artificial Intelligence:**\n",
        "\n",
        "1. Data-Driven Learning: Deep learning models excel at learning from large datasets without requiring explicit programming for feature extraction.\n",
        "\n",
        "2. High Accuracy: With the right architecture and enough data, deep learning models can achieve human-level or superhuman performance in tasks like image classification, language translation, and speech recognition.\n",
        "\n",
        "3. Versatility: It can be applied to a wide range of tasks across different domains, from recognizing faces in photos to generating human-like text or identifying diseases from medical images."
      ],
      "metadata": {
        "id": "yh00QESWaCXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. List and explain the fundamental components of artificial neural networks.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the ANN model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(28*28,)),  # Flatten input images to 1D\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')  # 10 classes for MNIST digits\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train.reshape(-1, 28*28), y_train, epochs=5, batch_size=64, validation_data=(x_test.reshape(-1, 28*28), y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test.reshape(-1, 28*28), y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S78KpP3UZ-Ku",
        "outputId": "95c26d6c-2b51-4636-f6b9-1ab5d902330a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8547 - loss: 0.4998 - val_accuracy: 0.9576 - val_loss: 0.1405\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9640 - loss: 0.1198 - val_accuracy: 0.9707 - val_loss: 0.0951\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9762 - loss: 0.0783 - val_accuracy: 0.9723 - val_loss: 0.0898\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9827 - loss: 0.0564 - val_accuracy: 0.9794 - val_loss: 0.0727\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9857 - loss: 0.0447 - val_accuracy: 0.9774 - val_loss: 0.0736\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9723 - loss: 0.0880\n",
            "Test accuracy: 0.977400004863739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Artificial neural networks (ANNs) are composed of several fundamental components that work together to model complex data patterns. Below are the key components:\n",
        "\n",
        "1. Neurons (Nodes):\n",
        "Neurons are the basic units of an artificial neural network, inspired by biological neurons. Each neuron takes one or more inputs, processes them, and produces an output. In a neural network, a neuron performs a weighted sum of its inputs and then applies an activation function to produce an output.\n",
        "\n",
        "2. Layers:\n",
        "\n",
        "- Input Layer: This layer receives the input data (features) and passes it to the next layer. Each neuron in the input layer corresponds to one feature.\n",
        "\n",
        "- Hidden Layers: These are layers between the input and output layers, where the actual computation happens. They allow the network to learn complex patterns. ANNs can have multiple hidden layers, and the deeper the network (i.e., the more hidden layers), the more powerful it can be.\n",
        "\n",
        "- Output Layer: This layer produces the final output or prediction of the network. For classification tasks, each neuron in the output layer corresponds to a class label.\n",
        "\n",
        "3. Weights and Biases:\n",
        "\n",
        "- Weights: These are parameters associated with each connection between neurons. The weight determines the strength of the signal passed from one neuron to another.\n",
        "\n",
        "- Biases: Bias values allow the network to make adjustments to the weighted sum of inputs before passing it through the activation function, helping the network learn patterns more effectively.\n",
        "\n",
        "4. Activation Function:\n",
        "The activation function determines whether a neuron should be activated or not. It introduces non-linearity into the model, allowing it to learn more complex patterns. Common activation functions include:\n",
        "\n",
        "- ReLU (Rectified Linear Unit): max(0, x)\n",
        "\n",
        "- Sigmoid: 1 / (1 + exp(-x))\n",
        "\n",
        "- Softmax: Used in the output layer for classification tasks, turning raw scores into probabilities.\n",
        "\n",
        "5. Forward Propagation:\n",
        "\n",
        "  Forward propagation refers to the process of passing input data through the network, layer by layer, to get the final output. The input is transformed at each layer by weighted sums and activation functions.\n",
        "\n",
        "6. Loss Function:\n",
        "\n",
        "  The loss function measures how far the network's predictions are from the actual values. Common loss functions include:\n",
        "\n",
        "- Mean Squared Error (MSE): Used in regression tasks.\n",
        "\n",
        "- Categorical Crossentropy: Used in classification tasks.\n",
        "\n",
        "7. Backpropagation:\n",
        "Backpropagation is the process of updating the weights and biases using the error between the predicted output and the actual output. It uses optimization algorithms like gradient descent to minimize the loss function."
      ],
      "metadata": {
        "id": "9wsFtClSaphe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Discuss the roles of neurons, connections, weights, and biases.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the ANN model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(28*28,)),  # Neurons in the input layer\n",
        "    Dense(64, activation='relu'),  # Neurons in the hidden layer\n",
        "    Dense(10, activation='softmax')  # Neurons in the output layer\n",
        "])\n",
        "\n",
        "# Compile the model (Optimizer adjusts weights and biases during training)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model (During training, weights and biases are updated)\n",
        "model.fit(x_train.reshape(-1, 28*28), y_train, epochs=5, batch_size=64, validation_data=(x_test.reshape(-1, 28*28), y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test.reshape(-1, 28*28), y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJb6EBWIa9Ww",
        "outputId": "10235d3e-8f63-4f9d-994c-e1577d930b2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.8617 - loss: 0.4890 - val_accuracy: 0.9567 - val_loss: 0.1457\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9631 - loss: 0.1255 - val_accuracy: 0.9685 - val_loss: 0.1059\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9752 - loss: 0.0821 - val_accuracy: 0.9694 - val_loss: 0.0982\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9807 - loss: 0.0608 - val_accuracy: 0.9747 - val_loss: 0.0837\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9865 - loss: 0.0441 - val_accuracy: 0.9756 - val_loss: 0.0813\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9719 - loss: 0.0960\n",
            "Test accuracy: 0.975600004196167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Roles of Neurons, Connections, Weights, and Biases in Artificial Neural Networks**\n",
        "\n",
        "1. Neurons:\n",
        "\n",
        " - Neurons are the fundamental building blocks of artificial neural networks. They are the computational units that process input data and pass the results to the next layer of neurons. Each neuron takes one or more inputs, applies a transformation to them, and produces an output. Neurons in the input layer represent raw data features, while those in the hidden and output layers help the network learn and generate predictions.\n",
        "\n",
        " - Each neuron performs a weighted sum of its inputs and applies an activation function to generate its output.\n",
        "\n",
        "2. Connections:\n",
        "\n",
        " - Connections are the links between neurons across layers. Each connection transmits the output of one neuron to another neuron in the subsequent layer. In a fully connected network, each neuron is connected to all neurons in the next layer.\n",
        "\n",
        " - Connections carry the weighted values, which indicate the importance of each input to the neuron it is connected to.\n",
        "\n",
        "3. Weights:\n",
        "\n",
        " - Weights represent the strength or importance of the connections between neurons. During training, the network learns the optimal weight values to minimize the error in predictions.\n",
        "\n",
        " - Each weight is associated with a specific connection between neurons and determines the contribution of that input to the neuron's output. Higher weights indicate that the corresponding input has a higher influence on the output.\n",
        "\n",
        " - Weights are updated during the training process through backpropagation.\n",
        "\n",
        "4. Biases:\n",
        "\n",
        " - Biases are added to the weighted sum of inputs before passing the result through the activation function. The bias allows the model to fit the data better by adjusting the output of the neuron independently of the input values.\n",
        "\n",
        " - Without biases, the network would be limited in the patterns it could learn. Biases allow neurons to activate even when all inputs are zero, improving the network's flexibility."
      ],
      "metadata": {
        "id": "ozxycSGNbepI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Illustrate the architecture of an artificial neural network. Provide an example to explain the flow of information through the network.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255  # Normalize input data\n",
        "x_test = x_test.astype('float32') / 255\n",
        "y_train = to_categorical(y_train, 10)  # One-hot encode labels\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the neural network architecture\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(28*28,)),  # Input layer (flattened 28x28 images) to hidden layer\n",
        "    Dense(64, activation='relu'),  # Hidden layer with 64 neurons\n",
        "    Dense(10, activation='softmax')  # Output layer with 10 neurons (for 10 class classification)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model (forward propagation occurs here)\n",
        "model.fit(x_train.reshape(-1, 28*28), y_train, epochs=5, batch_size=64, validation_data=(x_test.reshape(-1, 28*28), y_test))\n",
        "\n",
        "# Evaluate the model (evaluates accuracy of predictions)\n",
        "test_loss, test_accuracy = model.evaluate(x_test.reshape(-1, 28*28), y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLXFvghXb0Mc",
        "outputId": "2edab5ee-7b7f-433c-e12c-bc3a7dd1c759"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8570 - loss: 0.5128 - val_accuracy: 0.9603 - val_loss: 0.1377\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9661 - loss: 0.1149 - val_accuracy: 0.9656 - val_loss: 0.1061\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9761 - loss: 0.0784 - val_accuracy: 0.9715 - val_loss: 0.0915\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9824 - loss: 0.0575 - val_accuracy: 0.9723 - val_loss: 0.0889\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9868 - loss: 0.0428 - val_accuracy: 0.9775 - val_loss: 0.0756\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9751 - loss: 0.0898\n",
            "Test accuracy: 0.9775000214576721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Architecture of an Artificial Neural Network**\n",
        "\n",
        "An artificial neural network (ANN) consists of multiple layers of neurons, where each layer is fully connected to the previous and next layers. The architecture typically includes:\n",
        "\n",
        "1. Input Layer: This layer accepts the raw input data. Each neuron in this layer represents one feature of the input.\n",
        "\n",
        "2. Hidden Layers: These layers perform computations using neurons and their connections (with weights and biases) to transform the input data into more abstract representations. The number of hidden layers and neurons in each layer can vary.\n",
        "\n",
        "3. Output Layer: This layer generates the final predictions or outputs. In a classification task, it produces class probabilities, and in regression, it produces continuous values.\n",
        "\n",
        "4. Connections: Neurons in one layer are connected to neurons in the next layer through weighted connections.\n",
        "\n",
        "5. Weights and Biases: Each connection has a weight, and each neuron has a bias. These parameters are learned during training.\n",
        "\n",
        "6. Activation Function: Each neuron applies an activation function (like ReLU or Sigmoid) to its weighted sum of inputs to determine if it should \"fire\" and pass information to the next layer.\n",
        "\n",
        "**Flow of Information Through the Network**\n",
        "\n",
        "1. Forward Propagation: Information flows from the input layer through the hidden layers to the output layer. In each layer, the neurons process the inputs by computing a weighted sum and applying an activation function.\n",
        "\n",
        "2. Output: The output layer generates the final prediction or classification.\n",
        "\n",
        "3. Loss Calculation: The loss function compares the network's output with the true label, computing the error.\n",
        "\n",
        "4. Backpropagation: The error is propagated backward to adjust the weights and biases during training to minimize the loss."
      ],
      "metadata": {
        "id": "-4KGx7xMcJph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Outline the perceptron learning algorithm. Describe how weights are adjusted during the learning process.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Training data: OR gate (input, output)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input data\n",
        "y = np.array([0, 1, 1, 1])  # Output data\n",
        "\n",
        "# Initialize weights and bias\n",
        "weights = np.random.randn(2)\n",
        "bias = np.random.randn()\n",
        "learning_rate = 0.1\n",
        "epochs = 1000\n",
        "\n",
        "# Perceptron Learning Algorithm\n",
        "for epoch in range(epochs):\n",
        "    for i in range(len(X)):\n",
        "        # Compute the weighted sum\n",
        "        z = np.dot(X[i], weights) + bias\n",
        "        prediction = 1 if z >= 0 else 0  # Activation function (step function)\n",
        "\n",
        "        # Update weights and bias if the prediction is wrong\n",
        "        if prediction != y[i]:\n",
        "            weights += learning_rate * (y[i] - prediction) * X[i]\n",
        "            bias += learning_rate * (y[i] - prediction)\n",
        "\n",
        "print(\"Trained weights:\", weights)\n",
        "print(\"Trained bias:\", bias)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHw-mW3ncenr",
        "outputId": "887e0a79-3dd3-4e1d-f818-2efbcb6cfa60"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained weights: [0.12924052 0.22680065]\n",
            "Trained bias: -0.03872210417555352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perceptron Learning Algorithm**\n",
        "\n",
        "The perceptron learning algorithm is used to train a binary classifier called the perceptron. The algorithm updates the weights of the model based on the error between the predicted output and the actual output. The process is as follows:\n",
        "\n",
        "1. Initialize Weights: Start with random initial weights and bias.\n",
        "\n",
        "2. For Each Training Sample:\n",
        "\n",
        "    - Compute the weighted sum of the inputs: z = w1*x1 + w2*x2 + ... + wn*xn + b\n",
        "\n",
        "    - Apply the activation function (usually a step function): output = 1 if z >= 0 else 0\n",
        "\n",
        "3. Update Weights:\n",
        "\n",
        "    - If the prediction is correct (predicted == actual), no weight adjustment is made.\n",
        "\n",
        "    - If the prediction is incorrect (predicted != actual), adjust the weights and bias:\n",
        "      - Weight Update: wi = wi + learning_rate * (actual - predicted) * xi\n",
        "\n",
        "      - Bias Update: b = b + learning_rate * (actual - predicted)\n",
        "\n",
        "4. Repeat: This process is repeated for a fixed number of epochs or until the algorithm converges.\n",
        "\n",
        "**Weight Adjustment**\n",
        "\n",
        "The weight update is based on the error between the predicted and actual output. If the perceptron makes a wrong prediction, the weights are adjusted in such a way that the model will perform better on the next prediction. The learning rate controls the magnitude of the weight update."
      ],
      "metadata": {
        "id": "fPC1wl30dFVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Discuss the importance of activation functions in the hidden layers of a multi-layer perceptron.\n",
        "# Provide examples of commonly used activation functions.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build a neural network model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(28*28,)),  # Hidden layer with ReLU\n",
        "    Dense(64, activation='sigmoid'),  # Hidden layer with Sigmoid\n",
        "    Dense(10, activation='softmax')  # Output layer with Softmax for multi-class classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train.reshape(-1, 28*28), y_train, epochs=5, batch_size=64, validation_data=(x_test.reshape(-1, 28*28), y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test.reshape(-1, 28*28), y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jahMUSptfusi",
        "outputId": "2b12fa8d-1281-47da-d3a2-a80e1381322d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8113 - loss: 0.7332 - val_accuracy: 0.9450 - val_loss: 0.1884\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9516 - loss: 0.1656 - val_accuracy: 0.9636 - val_loss: 0.1212\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.1041 - val_accuracy: 0.9713 - val_loss: 0.0950\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9785 - loss: 0.0744 - val_accuracy: 0.9749 - val_loss: 0.0846\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9839 - loss: 0.0549 - val_accuracy: 0.9775 - val_loss: 0.0739\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9722 - loss: 0.0855\n",
            "Test accuracy: 0.9775000214576721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of Activation Functions in Hidden Layers**\n",
        "\n",
        "Activation functions play a crucial role in introducing non-linearity into the network, enabling it to model complex patterns. Without activation functions, a multi-layer perceptron (MLP) would essentially be a linear regression model, regardless of the number of layers. This would limit the network's ability to learn and represent intricate data patterns.\n",
        "\n",
        "In the hidden layers of a multi-layer perceptron, activation functions allow the model to:\n",
        "\n",
        "1. Introduce Non-linearity: This helps the model learn complex, non-linear relationships.\n",
        "\n",
        "2. Enable Backpropagation: They allow the use of gradient-based optimization techniques like backpropagation by providing useful gradients.\n",
        "\n",
        "3. Control Output Range: Some activation functions scale the output to specific ranges, which can help improve convergence during training.\n",
        "\n",
        "**Commonly Used Activation Functions**\n",
        "\n",
        "1. ReLU (Rectified Linear Unit):\n",
        "\n",
        "     - Formula: f(x) = max(0, x)\n",
        "\n",
        "     - It is the most widely used activation function due to its simplicity and effectiveness in training deep networks. It introduces non-linearity and avoids vanishing gradients for positive values.\n",
        "\n",
        "2. Sigmoid:\n",
        "\n",
        "     - Formula: f(x) = 1 / (1 + exp(-x))\n",
        "\n",
        "     - Sigmoid squashes the output between 0 and 1, making it useful for binary classification tasks. However, it suffers from the vanishing gradient problem for large values of x.\n",
        "\n",
        "3. Tanh (Hyperbolic Tangent):\n",
        "\n",
        "     - Formula: f(x) = (2 / (1 + exp(-2x))) - 1\n",
        "\n",
        "     - Tanh scales the output between -1 and 1, providing a wider range than the sigmoid. It also suffers from vanishing gradients but generally performs better than sigmoid in practice.\n",
        "\n",
        "4. Softmax (typically used in the output layer for classification):\n",
        "\n",
        "    - Formula: f(x_i) = exp(x_i) / sum(exp(x_j)) for all j\n",
        "\n",
        "    - Softmax converts raw scores into probabilities and is used in the output layer for multi-class classification tasks."
      ],
      "metadata": {
        "id": "ikiFmEWcgNaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Various Neural Network Architect Overview Assignments**"
      ],
      "metadata": {
        "id": "xgulKVHahttf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Describe the basic structure of a Feedforward Neural Network (FNN). What is the purpose of the activation function?\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the Feedforward Neural Network\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(28*28,)),  # Input to first hidden layer with ReLU\n",
        "    Dense(64, activation='relu'),  # Hidden layer with ReLU\n",
        "    Dense(10, activation='softmax')  # Output layer with Softmax for multi-class classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train.reshape(-1, 28*28), y_train, epochs=5, batch_size=64, validation_data=(x_test.reshape(-1, 28*28), y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test.reshape(-1, 28*28), y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvbonVGNhT9l",
        "outputId": "1e642dd3-c7fd-4fc3-f586-ab03036ba30d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8644 - loss: 0.4891 - val_accuracy: 0.9586 - val_loss: 0.1384\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9635 - loss: 0.1259 - val_accuracy: 0.9687 - val_loss: 0.1022\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9747 - loss: 0.0841 - val_accuracy: 0.9735 - val_loss: 0.0888\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9811 - loss: 0.0614 - val_accuracy: 0.9746 - val_loss: 0.0850\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9861 - loss: 0.0451 - val_accuracy: 0.9775 - val_loss: 0.0784\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9741 - loss: 0.0888\n",
            "Test accuracy: 0.9775000214576721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basic Structure of a Feedforward Neural Network (FNN)**\n",
        "\n",
        "A Feedforward Neural Network (FNN) consists of multiple layers, where the information flows in one direction—from the input layer to the output layer. It is called \"feedforward\" because the data moves forward through the network without any loops or cycles.\n",
        "\n",
        "1. Input Layer: The input layer receives the raw input data. Each neuron in this layer represents one feature of the data.\n",
        "\n",
        "2. Hidden Layers: These are intermediate layers that process the data by performing computations. Each neuron in a hidden layer takes inputs from the previous layer, applies weights and biases, and then passes the result through an activation function.\n",
        "\n",
        "3. Output Layer: The output layer produces the final predictions or classifications. It generates the result based on the transformations made in the hidden layers.\n",
        "\n",
        "4. Connections: Neurons in each layer are connected to neurons in the next layer through weighted connections.\n",
        "\n",
        "5. Weights and Biases: Each connection has a weight, and each neuron has a bias that helps adjust the output.\n",
        "\n",
        "**Purpose of the Activation Function**\n",
        "\n",
        "The activation function introduces non-linearity into the network. Without it, the network would only perform linear transformations, limiting its ability to model complex patterns in the data. The activation function allows the network to learn complex, non-linear relationships between inputs and outputs."
      ],
      "metadata": {
        "id": "jewFIf5eh_FR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Explain the role of convolutional layers in CNN. Why are pooling layers commonly used, and what do they achieve?\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)  # Reshape for CNN (28x28x1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)  # Reshape for CNN (28x28x1)\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build a CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer\n",
        "    MaxPooling2D(pool_size=(2, 2)),  # Pooling layer (Max pooling)\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),  # Convolutional layer\n",
        "    MaxPooling2D(pool_size=(2, 2)),  # Pooling layer (Max pooling)\n",
        "    Flatten(),  # Flatten the feature maps into a vector\n",
        "    Dense(128, activation='relu'),  # Fully connected layer\n",
        "    Dense(10, activation='softmax')  # Output layer (Softmax for multi-class classification)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzsWfsfwiJ8V",
        "outputId": "99f150d1-b8b1-4e4d-db04-63f90241b142"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.8907 - loss: 0.3623 - val_accuracy: 0.9810 - val_loss: 0.0581\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9861 - loss: 0.0464 - val_accuracy: 0.9821 - val_loss: 0.0574\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9895 - loss: 0.0318 - val_accuracy: 0.9890 - val_loss: 0.0346\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9920 - loss: 0.0244 - val_accuracy: 0.9884 - val_loss: 0.0330\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9942 - loss: 0.0172 - val_accuracy: 0.9866 - val_loss: 0.0418\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9837 - loss: 0.0516\n",
            "Test accuracy: 0.9865999817848206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Role of Convolutional Layers in CNN**\n",
        "\n",
        "Convolutional layers are the core building blocks of Convolutional Neural Networks (CNNs). They apply filters (kernels) to the input data (such as images) to detect local patterns, such as edges, textures, or other features. Each filter slides across the input (a process called convolution), creating feature maps that highlight specific patterns.\n",
        "\n",
        "The convolutional layers enable CNNs to:\n",
        "\n",
        "1. Detect Local Features: They capture spatial hierarchies and local features in images.\n",
        "\n",
        "2. Learn Spatial Patterns: Convolutional layers can detect patterns regardless of their position in the image.\n",
        "\n",
        "3. Reduce Parameters: By sharing weights across the image, convolutional layers reduce the number of parameters compared to fully connected layers.\n",
        "\n",
        "**Why Pooling Layers are Commonly Used and What They Achieve**\n",
        "\n",
        "Pooling layers are used after convolutional layers to downsample the feature maps, reducing their spatial dimensions while preserving important information. This serves several purposes:\n",
        "\n",
        "1. Dimensionality Reduction: Pooling reduces the number of parameters, decreasing computation and memory usage.\n",
        "\n",
        "2. Translation Invariance: It helps the network become invariant to small translations of the input.\n",
        "\n",
        "3. Feature Extraction: Pooling retains the most significant features while discarding less important information.\n",
        "\n",
        "**Common Types of Pooling**\n",
        "\n",
        " - Max Pooling: Takes the maximum value from a specific region (most commonly used).\n",
        "\n",
        " - Average Pooling: Takes the average value from a region."
      ],
      "metadata": {
        "id": "X14FrfdLitrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. What is the key characteristic that differentiates Recurrent Neural Networks (RNNs) from other neural networks?\n",
        "# How does an RNN handle sequential data?\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load and preprocess IMDb dataset (sentiment analysis)\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "x_train = pad_sequences(x_train, padding='post', maxlen=500)\n",
        "x_test = pad_sequences(x_test, padding='post', maxlen=500)\n",
        "\n",
        "# Reshape the data to be compatible with the RNN (add the features dimension)\n",
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "# Build an RNN model\n",
        "model = Sequential([\n",
        "    SimpleRNN(128, activation='tanh', input_shape=(500, 1)),  # RNN layer\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpuL2zeDjCn4",
        "outputId": "0fae4006-dc2f-460d-fefd-0c4d9c205244"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.5011 - loss: 0.6995 - val_accuracy: 0.4998 - val_loss: 0.6948\n",
            "Epoch 2/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 73ms/step - accuracy: 0.5005 - loss: 0.6966 - val_accuracy: 0.5017 - val_loss: 0.6961\n",
            "Epoch 3/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 73ms/step - accuracy: 0.4970 - loss: 0.6970 - val_accuracy: 0.4998 - val_loss: 0.6962\n",
            "Epoch 4/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 75ms/step - accuracy: 0.5000 - loss: 0.6957 - val_accuracy: 0.4961 - val_loss: 0.6971\n",
            "Epoch 5/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 74ms/step - accuracy: 0.4942 - loss: 0.6964 - val_accuracy: 0.5008 - val_loss: 0.7041\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.4961 - loss: 0.7049\n",
            "Test accuracy: 0.5008000135421753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Characteristic that Differentiates RNNs**\n",
        "\n",
        "The key characteristic that differentiates Recurrent Neural Networks (RNNs) from other neural networks is their ability to maintain memory of previous inputs through feedback loops. Unlike traditional feedforward neural networks, where information flows in one direction, RNNs have connections that loop back on themselves, allowing them to retain information about past inputs in the network's hidden states.\n",
        "\n",
        "This feedback mechanism makes RNNs well-suited for handling sequential data, as they can process inputs one at a time while maintaining context from previous time steps. This feature allows RNNs to model temporal dependencies and capture patterns in sequences, such as in time series or natural language.\n",
        "\n",
        "**How RNNs Handle Sequential Data**\n",
        "\n",
        "RNNs process sequential data by taking each element in the sequence and updating its hidden state based on both the current input and the previous hidden state. This allows the network to retain context from earlier elements in the sequence, enabling it to model dependencies across time steps."
      ],
      "metadata": {
        "id": "fTHt4-FajVFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.  Discuss the components of a Long Short-Term Memory (LSTM) network. How does it address the vanishing gradient problem?\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load and preprocess IMDb dataset (sentiment analysis)\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "x_train = pad_sequences(x_train, padding='post', maxlen=500)\n",
        "x_test = pad_sequences(x_test, padding='post', maxlen=500)\n",
        "\n",
        "# Build an LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(128, activation='tanh', input_shape=(500, 1)),  # LSTM layer\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJRkMR3tjZh0",
        "outputId": "ee8d2d8a-f771-436a-b28c-08b1a64f91e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 32ms/step - accuracy: 0.5014 - loss: 0.6942 - val_accuracy: 0.5074 - val_loss: 0.6923\n",
            "Epoch 2/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.5065 - loss: 0.6925 - val_accuracy: 0.5066 - val_loss: 0.6926\n",
            "Epoch 3/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 36ms/step - accuracy: 0.5095 - loss: 0.6931 - val_accuracy: 0.5012 - val_loss: 0.6924\n",
            "Epoch 4/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 31ms/step - accuracy: 0.5081 - loss: 0.6922 - val_accuracy: 0.5080 - val_loss: 0.6922\n",
            "Epoch 5/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 32ms/step - accuracy: 0.5132 - loss: 0.6917 - val_accuracy: 0.5008 - val_loss: 0.6928\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.4935 - loss: 0.6928\n",
            "Test accuracy: 0.5008000135421753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Components of a Long Short-Term Memory (LSTM) Network**\n",
        "\n",
        "An LSTM network is a special type of Recurrent Neural Network (RNN) designed to capture long-term dependencies in sequential data. It consists of three main components, each responsible for regulating information flow:\n",
        "\n",
        "1. Forget Gate: Decides which information from the previous timestep should be discarded from the cell state.\n",
        "\n",
        "2. Input Gate: Updates the cell state with new information by determining how much of the current input should be stored in the cell.\n",
        "\n",
        "3. Output Gate: Decides what the next hidden state (output) should be, based on the cell state and the current input.\n",
        "These components allow LSTMs to decide what information is important to keep, forget, and pass forward, making them highly effective for sequential tasks.\n",
        "\n",
        "**Addressing the Vanishing Gradient Problem**\n",
        "\n",
        "The vanishing gradient problem occurs in traditional RNNs when gradients become too small as they are propagated back through time, making it difficult to learn long-term dependencies. LSTMs address this issue with the cell state, which acts like a \"conveyor belt\" that can carry information across many timesteps with little modification. The forget and input gates regulate the flow of information, allowing the network to preserve important information over long sequences and mitigate the vanishing gradient problem."
      ],
      "metadata": {
        "id": "Ex8kxbRSkDFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Describe the roles of the generator and discriminator in a Generative Adversarial Network (GAN). What is the training objective for each?\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "# Generator model\n",
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=100, activation='relu'))\n",
        "    model.add(Dense(784, activation='sigmoid'))  # 28x28 image flattened\n",
        "    return model\n",
        "\n",
        "# Discriminator model\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=784, activation='relu'))  # 28x28 image flattened\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Combined GAN model\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    model = Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    return model\n",
        "\n",
        "# Compile the models\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator.trainable = False  # Freeze discriminator during generator training\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
        "\n",
        "# Training process (simplified)\n",
        "def train_gan(epochs=1, batch_size=128):\n",
        "    for epoch in range(epochs):\n",
        "        # Generate fake images\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "        fake_images = generator.predict(noise)\n",
        "\n",
        "        # Get real images (using random data here as a placeholder)\n",
        "        real_images = np.random.rand(batch_size, 784)  # Replace with actual data\n",
        "\n",
        "        # Train discriminator\n",
        "        real_labels = np.ones((batch_size, 1))\n",
        "        fake_labels = np.zeros((batch_size, 1))\n",
        "        d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
        "\n",
        "        # Train generator\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "        g_loss = gan.train_on_batch(noise, real_labels)  # Try to fool the discriminator\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, D Loss: {d_loss_real[0] + d_loss_fake[0]}, G Loss: {g_loss}\")\n",
        "\n",
        "# Train the GAN\n",
        "train_gan(epochs=5, batch_size=128)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O9K7-IukDwk",
        "outputId": "97dc3329-5623-4e09-c5df-a85e9333d261"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:75: UserWarning: The model does not have any trainable weights.\n",
            "  warnings.warn(\"The model does not have any trainable weights.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, D Loss: 1.2769410610198975, G Loss: [array(0.6879482, dtype=float32), array(0.6879482, dtype=float32), array(0.40625, dtype=float32)]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 1959 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7d508558e700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 1960 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7d508558e700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, D Loss: 1.4036948680877686, G Loss: [array(0.74562377, dtype=float32), array(0.74562377, dtype=float32), array(0.3828125, dtype=float32)]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "Epoch 3/5, D Loss: 1.5164157152175903, G Loss: [array(0.80256623, dtype=float32), array(0.80256623, dtype=float32), array(0.3841146, dtype=float32)]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "Epoch 4/5, D Loss: 1.6402628421783447, G Loss: [array(0.8673217, dtype=float32), array(0.8673217, dtype=float32), array(0.38183594, dtype=float32)]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Epoch 5/5, D Loss: 1.7682764530181885, G Loss: [array(0.9314845, dtype=float32), array(0.9314845, dtype=float32), array(0.38125, dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Roles of the Generator and Discriminator in a GAN**\n",
        "\n",
        "In a Generative Adversarial Network (GAN), there are two main components:\n",
        "\n",
        "1. Generator: The generator's role is to create synthetic data that resembles the real data distribution. It takes random noise as input and produces data (such as images) that should be indistinguishable from real data.\n",
        "\n",
        "2. Discriminator: The discriminator's role is to distinguish between real and generated (fake) data. It takes an input (either real or generated data) and outputs a probability indicating whether the input is real or fake.\n",
        "\n",
        "**Training Objective for Each**\n",
        "\n",
        "- Generator's Objective: The generator aims to fool the discriminator by producing realistic data. It tries to minimize the discriminator's ability to distinguish between real and fake data.\n",
        "\n",
        "- Discriminator's Objective: The discriminator aims to correctly classify data as real or fake. It tries to maximize its ability to differentiate between real and generated data.\n",
        "\n",
        "Both networks are trained simultaneously, with the generator improving its data generation, and the discriminator improving its ability to classify data correctly. This adversarial process continues until the generator produces data that is nearly indistinguishable from real data, and the discriminator can no longer distinguish between the two."
      ],
      "metadata": {
        "id": "jDG4iQc6kEUL"
      }
    }
  ]
}